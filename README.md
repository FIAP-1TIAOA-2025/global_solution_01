# PrevisÃ£o de InundaÃ§Ãµes no Brasil (POC)

Este projeto Ã© uma POC (Proof Of Concept) que visa desenvolver um modelo de InteligÃªncia Artificial para prever eventos de inundaÃ§Ãµes utilizando dados climÃ¡ticos e, potencialmente, dados hidrolÃ³gicos e geogrÃ¡ficos. O objetivo Ã© fornecer uma ferramenta de alerta precoce que possa auxiliar na gestÃ£o de riscos e na proteÃ§Ã£o da populaÃ§Ã£o.

---

## ğŸš€ VisÃ£o Geral do Projeto

InundaÃ§Ãµes sÃ£o desastres naturais recorrentes em muitas cidades costeiras e tropicais, como Salvador. Prever esses eventos com antecedÃªncia pode salvar vidas e reduzir danos materiais. Este projeto explora o uso de tÃ©cnicas de Machine Learning, especificamente com Python e Scikit-learn, para identificar padrÃµes em dados climÃ¡ticos que precedem inundaÃ§Ãµes.

O pipeline do projeto segue uma abordagem de ciÃªncia de dados estruturada:

1. Coleta e PreparaÃ§Ã£o de Dados: AquisiÃ§Ã£o de dados climÃ¡ticos histÃ³ricos, registros de inundaÃ§Ãµes e outras informaÃ§Ãµes relevantes.
2. AnÃ¡lise ExploratÃ³ria de Dados (EDA): Entendimento inicial dos dados, identificaÃ§Ã£o de tendÃªncias, sazonalidade e problemas de qualidade.
3. Engenharia de Features: CriaÃ§Ã£o de variÃ¡veis mais informativas (ex: precipitaÃ§Ã£o acumulada, lags) a partir dos dados brutos.
4. Treinamento e AvaliaÃ§Ã£o do Modelo: SeleÃ§Ã£o, treinamento e otimizaÃ§Ã£o de modelos de Machine Learning para prever inundaÃ§Ãµes.
5. RelatÃ³rios e VisualizaÃ§Ãµes: GeraÃ§Ã£o de grÃ¡ficos e relatÃ³rios para comunicar insights e performance do modelo.

---

## ğŸ“‚ Estrutura do Projeto

A organizaÃ§Ã£o do projeto segue uma estrutura modular para facilitar o desenvolvimento, a manutenÃ§Ã£o e a colaboraÃ§Ã£o:

```text
flood_prediction_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Dados brutos originais (nunca modificados)
â”‚   â””â”€â”€ processed/           # Dados limpos e com features engenheiradas, prontos para o modelo
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda_and_initial_analysis.ipynb # EDA e anÃ¡lise de correlaÃ§Ã£o inicial
â”‚   â”œâ”€â”€ 02_feature_engineering_exploration.ipynb # ExploraÃ§Ã£o e criaÃ§Ã£o de features
â”‚   â””â”€â”€ 03_model_experimentation.ipynb    # Treinamento e avaliaÃ§Ã£o de modelos
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py    # Scripts para carregar/coletar dados brutos
â”‚   â”œâ”€â”€ data_preprocessing.py # FunÃ§Ãµes para limpeza e engenharia de features
â”‚   â”œâ”€â”€ model_training.py    # LÃ³gica para treinamento e tuning do modelo
â”‚   â”œâ”€â”€ model_evaluation.py  # FunÃ§Ãµes para mÃ©tricas e visualizaÃ§Ãµes de avaliaÃ§Ã£o
â”‚   â””â”€â”€ prediction_api.py    # (Opcional) API para servir prediÃ§Ãµes
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_models/      # Modelos treinados e scalers salvos
â”‚   â””â”€â”€ checkpoints/         # Saves intermediÃ¡rios de modelos (se aplicÃ¡vel)
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/             # GrÃ¡ficos e visualizaÃ§Ãµes gerados
â”‚   â”œâ”€â”€ final_report.md      # RelatÃ³rio final do projeto
â”‚   â””â”€â”€ model_performance_metrics.csv # MÃ©tricas de performance do modelo final
â”‚
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente (ex: credenciais, *NÃƒO comitar!*)
â”œâ”€â”€ requirements.txt         # Lista de dependÃªncias Python
â”œâ”€â”€ README.md                # Este arquivo
â””â”€â”€ run_pipeline.py          # Script principal para executar o pipeline completo
```

---

## ğŸ› ï¸ Tecnologias Utilizadas

- Python 3.x
- Bibliotecas Python:
  - pandas: ManipulaÃ§Ã£o e anÃ¡lise de dados
  - numpy: ComputaÃ§Ã£o numÃ©rica
  - scikit-learn: Modelagem de Machine Learning
  - matplotlib: VisualizaÃ§Ã£o de dados
  - seaborn: VisualizaÃ§Ã£o de dados estatÃ­sticos
  - imblearn: Para lidar com desbalanceamento de classes (SMOTE)
  - joblib: Para salvar e carregar modelos
  - python-dotenv: Para carregar variÃ¡veis de ambiente (se usar .env)
- Jupyter Notebook: Para exploraÃ§Ã£o e prototipagem
- MongoDB (Local/Atlas): Para armazenamento de dados (conforme sua escolha)

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

Siga estes passos para configurar o ambiente de desenvolvimento e executar o projeto:

1. Clonar o RepositÃ³rio:

    ```bash
    git clone https://github.com/seu_usuario/flood_prediction_project.git
    cd flood_prediction_project
    ```

2. Criar e Ativar Ambiente Virtual:
    Ã‰ altamente recomendado usar um ambiente virtual para isolar as dependÃªncias do projeto.

    ```bash
    python3 -m venv .venv
    # No macOS/Linux:
    source .venv/bin/activate
    # No Windows:
    .venv\Scripts\activate
    ```

3. Instalar DependÃªncias:

    ```bash
    pip install -r requirements.txt
    ```

4. Configurar MongoDB (Local):
    Se vocÃª optou por usar o MongoDB localmente:

    - Instale o MongoDB Community Server: Siga as instruÃ§Ãµes de instalaÃ§Ã£o para seu sistema operacional no site oficial do MongoDB.
    - Inicie o servidor MongoDB: Garanta que o processo mongod esteja rodando (geralmente na porta 27017).

5. VariÃ¡veis de Ambiente (Opcional, mas Recomendado):
    Se vocÃª for usar credenciais ou caminhos sensÃ­veis, crie um arquivo .env na raiz do projeto (o git o ignorarÃ¡) e defina suas variÃ¡veis lÃ¡.
    Exemplo de .env:

    ```text
    MONGO_URI="mongodb://localhost:27017/"
    # Ou sua string do Atlas se decidir voltar:
    # MONGO_URI="mongodb+srv://<username>:<password>@<cluster-url>/?retryWrites=true&w=majority"
    ```

---

## ğŸš€ Como Executar o Projeto

VocÃª pode executar o projeto de duas maneiras:

### A. Usando os Jupyter Notebooks (Para ExploraÃ§Ã£o)

Para entender cada etapa em detalhes e experimentar:

1. Inicie o Jupyter Lab/Notebook:

    ```bash
    jupyter lab # ou jupyter notebook
    ```

2. Navegue atÃ© a pasta ```notebooks/```.
3. Execute os notebooks em ordem numÃ©rica:
    - 01_eda_and_initial_analysis.ipynb
    - 02_feature_engineering_exploration.ipynb
    - 03_model_experimentation.ipynb

### B. Executando o Pipeline Completo (Para ReproduÃ§Ã£o e Testes)

Uma vez que as etapas dos notebooks foram prototipadas, a lÃ³gica principal Ã© refatorada para os scripts em src/ e orquestrada pelo run_pipeline.py.

1. Certifique-se de que seu ambiente virtual estÃ¡ ativado.
2. Execute o script principal:

    ```bash
    python run_pipeline.py
    ```

    Este script executarÃ¡ as etapas de carregamento de dados, prÃ©-processamento, engenharia de features, treinamento do modelo e avaliaÃ§Ã£o.

---

## ğŸ“Š Resultados e AnÃ¡lise

Os resultados das anÃ¡lises, grÃ¡ficos gerados e a performance do modelo final serÃ£o salvos na pasta ```reports/```.

- ```reports/figures/```: ContÃ©m as visualizaÃ§Ãµes (mapas de calor de correlaÃ§Ã£o, grÃ¡ficos de sÃ©ries temporais, matrizes de confusÃ£o, curvas ROC/PR).
- ```reports/model_performance_metrics.csv```: Um resumo das mÃ©tricas de avaliaÃ§Ã£o do modelo final.
- ```reports/final_report.md```: Um relatÃ³rio mais detalhado sobre a metodologia, achados e conclusÃµes do projeto.
